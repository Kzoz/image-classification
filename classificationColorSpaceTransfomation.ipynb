#!/usr/bin/env python
# coding: utf-8

# In[1]:


import numpy as np
import matplotlib.pyplot as plt
import os
import cv2
from tqdm import tqdm
from skimage.color import rgb2gray
get_ipython().run_line_magic('matplotlib', 'inline')
from scipy import ndimage


DATADIR = "/Users/ieuser/okraclassification/okra_image"

CATEGORIES = ["c_type", "d_type"]

for category in CATEGORIES:  
    path = os.path.join(DATADIR,category)  # create path 
    for img in os.listdir(path):  # iterate over each image 
        image = cv2.imread(os.path.join(path,img) )  # convert to array
        #plt.imshow(image, cmap='gray')  # graph it
        #plt.show()  # display!
        image.shape 
        plt.imshow(image)

        break  
    break  


# In[2]:


# save the cropped images in a different folder


# In[3]:


image= np.array(image)
   
#HSV_img = cv2.cvtColor(image, cv2.COLOR_BGR2HSV) 
HLS_img = cv2.cvtColor(image, cv2.COLOR_BGR2HLS) #CV_BGR2HLS equivalent to HSI
#image = cv2.cvtColor(image, cv2.COLOR_BGR2HLS)

# displaying the Hsv format image
plt.imshow(HLS_img)


# In[4]:


img_ycrcb = cv2.cvtColor(image, cv2.COLOR_BGR2YCR_CB)
plt.imshow(img_ycrcb)


# In[5]:


gray = rgb2gray(image)
plt.imshow(gray, cmap='gray')


# In[6]:


h,l,s = cv2.split(HLS_img)
r,g,b = cv2.split(image)
y,cr,cb = cv2.split(img_ycrcb)


# In[10]:


plt.imshow(cb)


# In[11]:


images = cv2.merge((h,l,g))
plt.imshow(images)


# In[ ]:





# In[12]:


new_array = image


# In[13]:


temp = np.zeros(new_array.shape, dtype='uint8')
temp[:,:,2] = new_array[:,:,2]


# In[14]:


plt.imshow(temp)


# In[15]:


gray = rgb2gray(new_array)
plt.imshow(gray, cmap='gray')


# In[16]:


gray.shape[1]


# In[17]:


gray_r = gray.reshape(gray.shape[0]*gray.shape[1])
np.amax(gray_r)


# In[18]:


gray_r = gray.reshape(gray.shape[0]*gray.shape[1])
for i in range(gray_r.shape[0]):
    if gray_r[i] > np.amax(gray_r)*0.44:
    #if gray_r[i] > gray_r.mean():
        gray_r[i] = 0
    else:
        gray_r[i] = 1
gray = gray_r.reshape(gray.shape[0],gray.shape[1])
plt.imshow(gray, cmap='gray')


# In[19]:


extract_img = np.zeros(new_array.shape, dtype='uint8')
for i in [0,1,2]:
    extract_img[:,:,i] = new_array[:,:,i]*gray
    plt.imshow(extract_img)


# In[20]:


IMG_LONG = 128
IMG_LARGE = 128

new_array = cv2.resize(images, (IMG_LONG, IMG_LARGE))
plt.imshow(new_array, cmap='gray')
plt.show()


# In[ ]:


training_data = []

def create_training_data():
    for category in CATEGORIES:  # do 

        path = os.path.join(DATADIR,category)  # create path
        class_num = CATEGORIES.index(category)  # get the classification  (0 or a 1). 

        for img in tqdm(os.listdir(path)):  # iterate over each image 
            try:
                image = cv2.imread(os.path.join(path,img))
                new_array = cv2.resize(image, (IMG_LONG, IMG_LARGE))
                gray = rgb2gray(new_array)
                gray_r = gray.reshape(gray.shape[0]*gray.shape[1])
                for i in range(gray_r.shape[0]):
                    if gray_r[i] > np.amax(gray_r)*0.44:
                    #if gray_r[i] > gray_r.mean():
                        gray_r[i] = 0
                    else:
                        gray_r[i] = 1
                gray = gray_r.reshape(gray.shape[0],gray.shape[1])
                extract_img = np.zeros(new_array.shape, dtype='uint8')
                for i in [0,1,2]:
                    extract_img[:,:,i] = new_array[:,:,i]*gray
                training_data.append([extract_img, class_num])
            except Exception as e:  # in the interest in keeping the output clean...
                pass
            
            
            
create_training_data()

print(len(training_data))


# In[35]:


#training_data = []

#def create_training_data():
#    for category in CATEGORIES:  # do 

#        path = os.path.join(DATADIR,category)  # create path
#        class_num = CATEGORIES.index(category)  # get the classification  (0 or a 1). 

#        for img in tqdm(os.listdir(path)):  # iterate over each image 
#            try:
#                img_array = cv2.imread(os.path.join(path,img))  # convert to array ->  ,cv2.IMREAD_GRAYSCALE for grayscaling
#                extract_img2 = cv2.resize(img_array, (IMG_LONG, IMG_LARGE))  # resize to normalize data size
#                training_data.append([extract_img2, class_num])  # add this to our training_data
#            except Exception as e:  # in the interest in keeping the output clean...
#                pass
#            #except OSError as e:
#            #    print("OSErrroBad img most likely", e, os.path.join(path,img))
#            #except Exception as e:
#            #    print("general exception", e, os.path.join(path,img))

#create_training_data()

#print(len(training_data))


# In[ ]:





# In[36]:


import random

random.shuffle(training_data)


# In[37]:


for sample in training_data[:10]:
    print(sample[1])


# In[38]:


plt.imshow(new_array, cmap='gray')
plt.show()


# In[39]:


plt.imshow(extract_img, cmap='gray')
plt.show()


# In[40]:


X = []
y = []

for features,label in training_data:
    X.append(features)
    y.append(label)

print(X[0].reshape(-1, IMG_LARGE, IMG_LONG, 3))

X = np.array(X).reshape(-1, IMG_LARGE, IMG_LONG, 3)


# In[41]:


X.shape


# In[42]:


len(y)


# In[43]:


import pickle

pickle_out = open("X.pickle","wb")
pickle.dump(X, pickle_out)
pickle_out.close()

pickle_out = open("y.pickle","wb")
pickle.dump(y, pickle_out)
pickle_out.close()


# In[44]:


#pickle_in = open("X.pickle","rb")
#X = pickle.load(pickle_in)

#pickle_in = open("y.pickle","rb")
#y = pickle.load(pickle_in) 


# In[46]:


import tensorflow as tf
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten
from tensorflow.keras.layers import Conv2D, MaxPooling2D

import pickle

pickle_in = open("X.pickle","rb")
X = pickle.load(pickle_in)

pickle_in = open("y.pickle","rb")
y = pickle.load(pickle_in)

X = X/255.0

model = Sequential()

model.add(Conv2D(256, (3, 3), input_shape=X.shape[1:]))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(256, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(256, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors

model.add(Dense(64))
model.add(Activation('relu'))

model.add(Dense(1))
model.add(Activation('sigmoid'))

model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy']
             )

model.fit(X, y, batch_size=32, epochs=5, validation_split=0.2)


# In[115]:


import tensorflow as tf
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten
from tensorflow.keras.layers import Conv2D, MaxPooling2D

import pickle

pickle_in = open("X.pickle","rb")
X = pickle.load(pickle_in)

pickle_in = open("y.pickle","rb")
y = pickle.load(pickle_in)

X = X/255.0

# split into train and test
n_test = 2000
trainX, testX = X[:n_test, :], X[n_test:, :]
trainy, testy = y[:n_test], y[n_test:]

model = Sequential()

model.add(Conv2D(32, (3, 3), input_shape=X.shape[1:]))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(32, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(32, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors

model.add(Dense(64))
model.add(Activation('relu'))

model.add(Dense(1))
model.add(Activation('sigmoid'))

model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy']
             )

#model.fit(X, y, batch_size=64, epochs=3, validation_split=0.2)

history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=5, verbose=0)

# evaluate the model
_, train_acc = model.evaluate(trainX, trainy, verbose=0)
_, test_acc = model.evaluate(testX, testy, verbose=0)
# plot loss during training
#Loss is defined as the difference between the predicted value by your model and the true value. The most common loss function used in deep neural networks is cross-entropy. 
pyplot.subplot(211)
pyplot.title('Loss')
pyplot.plot(history.history['loss'], label='train')
pyplot.plot(history.history['val_loss'], label='test')
pyplot.legend()
pyplot.show()
# plot accuracy during training
pyplot.subplot(212)
pyplot.title('Accuracy')
pyplot.plot(history.history['acc'], label='train')
pyplot.plot(history.history['val_acc'], label='test')
pyplot.legend()
pyplot.show()


# In[ ]:





# In[61]:


# demonstration of calculating metrics for a neural network model using sklearn
from sklearn.datasets import make_circles
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.metrics import cohen_kappa_score
from sklearn.metrics import roc_auc_score
from sklearn.metrics import confusion_matrix

 
pickle_in = open("X.pickle","rb")
X = pickle.load(pickle_in)

pickle_in = open("y.pickle","rb")
y = pickle.load(pickle_in)

X = X/255.0
# generate and prepare the dataset
def get_data():
    
# generate dataset
    n_test = 1950
    trainX, testX = X[:n_test, :], X[n_test:, :]
    trainy, testy = y[:n_test], y[n_test:]
    return trainX, trainy, testX, testy
 
# define and fit the model
def get_model(trainX, trainy):
    model = Sequential()

    model.add(Conv2D(256, (6, 6), input_shape=X.shape[1:]))
    model.add(Activation('relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))

    model.add(Conv2D(256, (3, 3)))
    model.add(Activation('relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    
    model.add(Conv2D(256, (3, 3)))
    model.add(Activation('relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))

    model.add(Flatten())

    model.add(Dense(64))
    model.add(Activation('relu'))

    model.add(Dense(1))
    model.add(Activation('sigmoid'))

    model.compile(loss='binary_crossentropy',
                  optimizer='adam',
                  metrics=['accuracy']
                 )

    model.fit(trainX, trainy, validation_data=(testX, testy), epochs=5, verbose=0)
    return model

# generate data
trainX, trainy, testX, testy = get_data()
# fit model
model = get_model(trainX, trainy)
 
 
# predict probabilities for test set
yhat_probs = model.predict(testX, verbose=0)
# predict crisp classes for test set
yhat_classes = model.predict_classes(testX, verbose=0)
# reduce to 1d array
yhat_probs = yhat_probs[:, 0]
yhat_classes = yhat_classes[:, 0]
 
# accuracy: (tp + tn) / (p + n)
accuracy = accuracy_score(testy, yhat_classes)
print('Accuracy: %f' % accuracy)
# precision tp / (tp + fp)
precision = precision_score(testy, yhat_classes)
print('Precision: %f' % precision)
# recall: tp / (tp + fn)
recall = recall_score(testy, yhat_classes)
print('Recall: %f' % recall)
# f1: 2 tp / (2 tp + fp + fn)
f1 = f1_score(testy, yhat_classes)
print('F1 score: %f' % f1)
 
# kappa
#kappa = cohen_kappa_score(testy, yhat_classes)
#print('Cohens kappa: %f' % kappa)
# ROC AUC
#auc = roc_auc_score(testy, yhat_probs)
#print('ROC AUC: %f' % auc)
# confusion matrix
matrix = confusion_matrix(testy, yhat_classes)
print(matrix)


# In[29]:


import tensorflow as tf
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten
from tensorflow.keras.layers import Conv2D, MaxPooling2D

import pickle

pickle_in = open("X.pickle","rb")
X = pickle.load(pickle_in)

pickle_in = open("y.pickle","rb")
y = pickle.load(pickle_in)

X = X/255.0

model = Sequential()

model.add(Conv2D(256, (3, 3), input_shape=X.shape[1:]))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(256, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(256, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors

model.add(Dense(64))
model.add(Activation('relu'))

model.add(Dense(1))
model.add(Activation('sigmoid'))

model.compile(loss='binary_crossentropy',
              optimizer='nadam',
              metrics=['accuracy']
             )

model.fit(X, y, batch_size=32, epochs=5, validation_split=0.2)


# In[33]:


X.shape


# In[93]:



# Example of generating samples from the two circle problem
from sklearn.datasets import make_circles
from matplotlib import pyplot
from numpy import where
# generate 2d classification dataset
X, y = make_circles(n_samples=9756, noise=0.1, random_state=1)
# scatter plot, dots colored by class value
for i in range(2):
	samples_ix = where(y == i)
	pyplot.scatter(X[samples_ix, 0], X[samples_ix, 1])
pyplot.show()


# In[ ]:





# In[ ]:





# In[ ]:





# In[ ]:





# In[15]:


#loss_and_metrics = model.evaluate(X, y, batch_size=128)


# In[42]:


import numpy as np
import pandas as pd

dataset = training_data

df = pd.DataFrame(dataset)

#dataset

#x = dataset.iloc[:, [0]]
#y = dataset.iloc[:, 1]

pickle_in = open("X.pickle","rb")
X = pickle.load(pickle_in)
x = np.array(X)
x = x.reshape(-1,46*47*3)


#x = x.reshape(-1,50*50)
#x = df[0].values
#x = x/255.0

pickle_in = open("y.pickle","rb")
y = pickle.load(pickle_in)
y = df[1].values


# In[44]:


import pandas as pd
import os
import numpy as np
from sklearn import metrics
from scipy.stats import zscore
from sklearn.model_selection import KFold
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation
from sklearn.model_selection import train_test_split


kf = KFold(5, shuffle=True, random_state=42) 

oos_y = []
oos_pred = []

fold = 0
#for train, test in kf.split(x):
for train, test in kf.split(x):
    fold+=1
    print(f"Fold #{fold}")
        
    x_train = x[train]
    y_train = y[train]
    x_test = x[test]
    y_test = y[test]
    
    model = Sequential()
    model.add(Dense(20, input_dim=x.shape[1], activation='relu'))
    model.add(Dense(10, activation='relu'))
    model.add(Dense(1, activation='sigmoid'))
    model.compile(loss='mean_squared_error', optimizer='adam')
    
    model.fit(x_train,y_train,validation_data=(x_test,y_test),verbose=0,epochs=50)
    
    pred = model.predict(x_test)
    
    oos_y.append(y_test)
    oos_pred.append(pred)    

    # Measure this fold's RMSE
    score = np.sqrt(metrics.mean_squared_error(pred,y_test))
    accuracy_score = metrics.accuracy_score(pred,y_test)
    print(f"Fold score (RMSE): {score}")
    print(f"Fold score (accuracy): {accuracy_score}")

# Build the oos prediction list and calculate the error.
oos_y = np.concatenate(oos_y)
oos_pred = np.concatenate(oos_pred)
score = np.sqrt(metrics.mean_squared_error(oos_pred,oos_y))

accuracy_score = metrics.accuracy_score(oos_pred,oos_y)
print(f"Final, out of sample score (RMSE): {score}")    
print(f"Final score (accuracy): {accuracy_score}")    


    
# Write the cross-validated prediction
oos_y = pd.DataFrame(oos_y)
oos_pred = pd.DataFrame(oos_pred)
oosDF = pd.concat( [df, oos_y, oos_pred],axis=1 )
#oosDF.to_csv(filename_write,index=False)


# In[75]:


np.unique(y_test)


# In[ ]:




